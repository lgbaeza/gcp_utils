{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d94ba7-0c49-484a-980a-ebb03cb4eccd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Deploy Dataplex Profiling and Quality\n",
    "\n",
    "- Luis Gerardo Baeza, Customer Engineer\n",
    "- Sep 23rd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab62b88-5324-44ad-999e-04b69e62deab",
   "metadata": {},
   "source": [
    "## 1. Preparation\n",
    "**For the principal responsible for running this script (e.g. Vertex Workbench service account):**\n",
    "- Grant BigQuery Data Editor IAM Role, or [minimum permissions](https://cloud.google.com/dataplex/docs/use-data-profiling#permissions) according to Docs, in each of the tables, datasets and GCP projects configured below\n",
    "- Grant Dataplex Data Scan Editor (roles/dataplex.dataScanEditor) or [minimum permissions](https://cloud.google.com/dataplex/docs/use-data-profiling#roles-permissions) according to Docs\n",
    "- This script will create Dataplex scan jobs in the GCP project in context\n",
    "- This script will inmediately run the jobs\n",
    "- The last section will modify the jobs to schedule the run\n",
    "\n",
    "This is a sample code, not meant for production. Please use with caution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1b1fc2-3fa8-49b9-b76f-91fd785009fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74f9000e-b071-49bb-a6b3-12b5429a3720",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade --quiet google-cloud-dataplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5a91c3-b16c-4b6e-b3bd-acf23c4fc074",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.api_core.exceptions\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import dataplex_v1\n",
    "import os\n",
    "from typing import List\n",
    "from google.protobuf import field_mask_pb2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148a885f-e293-4630-96be-fa50166f7846",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Provide the BigQuery projects that you want to include in the profiling and quality\n",
    "GOOGLE_CLOUD_PROJECT_ID_BQ_SOURCE = ['']\n",
    "GOOGLE_CLOUD_DATASET_TEST = ['test_dataset'] #empty list if processing everything\n",
    "DATAPLEX_LOCATION = \"us-central1\"\n",
    "DPLX_EXPORT_DATASET_ID = \"dplex_at_scale\"\n",
    "DPLEX_EXPORT_TABLE_NAME_profiling = \"profiling\"\n",
    "DPLEX_EXPORT_TABLE_NAME_quality = \"quality\"\n",
    "DPLEX_CRON_SCHEDULE = \"0 2 * * *\"\n",
    "sampling_percent = 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "083d30a2-b38b-4acd-93cd-c516d6fa3e73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GOOGLE_CLOUD_PROJECT_DATAPLEX = os.environ.get( 'GOOGLE_CLOUD_PROJECT' )\n",
    "BQ_DPLEX_EXPORT_TABLE = f\"projects/{GOOGLE_CLOUD_PROJECT_DATAPLEX}/datasets/{DPLX_EXPORT_DATASET_ID}/tables/{DPLEX_EXPORT_TABLE_NAME_profiling}\"\n",
    "BQ_DPLEX_EXPORT_TABLE_quality = f\"projects/{GOOGLE_CLOUD_PROJECT_DATAPLEX}/datasets/{DPLX_EXPORT_DATASET_ID}/tables/{DPLEX_EXPORT_TABLE_NAME_quality}\"\n",
    "dataplex_client = dataplex_v1.DataScanServiceClient()\n",
    "bq_client = bigquery.Client()\n",
    "dplex_service_client  = dataplex_v1.DataplexServiceClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c2a3316e-c5de-4e16-a715-db0057c0a668",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dataset for export\n",
    "SQL = f\"\"\"\n",
    "    CREATE SCHEMA IF NOT EXISTS  {DPLX_EXPORT_DATASET_ID};\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "faa7a130-69e3-4a70-bea2-f7a0363f8326",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0500d4a26d14a448e1e0eedc9be90af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "$SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc81be54-dc52-4523-bd43-c81379898e11",
   "metadata": {},
   "source": [
    "## 3. Dataplex deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a5a83e-c7d7-4a9d-b965-e5737ab69778",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1 Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe6d341-ad94-4e82-8e2a-96f7ef35d785",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.1.1 Create all dataplex profiling and quality jobs - main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92a209f-6396-4541-ae3b-25e6777e87db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataplex_table_profiling_jobs( ) :\n",
    "    \"\"\"\n",
    "    Creates Dataplex table profiling jobs for all datasets within a list of BigQuery projects' datasets.\n",
    "\n",
    "    Args:\n",
    "        bigquery_project_ids: A list of BigQuery project IDs to iterate through.\n",
    "        dataplex_project_id: The ID of the Dataplex project.\n",
    "        dataplex_location: The Google Cloud region for the Dataplex jobs (e.g., 'us-central1').\n",
    "    \"\"\"\n",
    "    for bigquery_project_id in GOOGLE_CLOUD_PROJECT_ID_BQ_SOURCE:\n",
    "        print(f\"\\nProcessing BigQuery project: {bigquery_project_id}\")\n",
    "        \n",
    "        dplx_parent = dataplex_client.common_location_path(\n",
    "            GOOGLE_CLOUD_PROJECT_DATAPLEX, DATAPLEX_LOCATION\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            # Initialize BigQuery client for the current project\n",
    "            bigquery_client = bigquery.Client(project=bigquery_project_id)\n",
    "            datasets = list(bigquery_client.list_datasets())\n",
    "\n",
    "            if not datasets:\n",
    "                print(f\"No datasets found in the BigQuery project '{bigquery_project_id}'.\")\n",
    "                continue\n",
    "\n",
    "            for dataset in datasets:\n",
    "                dataset_id = dataset.dataset_id\n",
    "                time.sleep(0.5)\n",
    "\n",
    "                if len(GOOGLE_CLOUD_DATASET_TEST) > 0:\n",
    "                    if dataset_id in GOOGLE_CLOUD_DATASET_TEST:\n",
    "                        pass\n",
    "                    else:\n",
    "                        print(f\"\\Skipping dataset: {dataset_id}\")\n",
    "                        continue\n",
    "                \n",
    "                print(f\"\\nProcessing dataset: {dataset_id}\")\n",
    "                tables = list(bigquery_client.list_tables(dataset))\n",
    "\n",
    "                if not tables:\n",
    "                    print(f\"No tables found in dataset: {dataset.dataset_id}\")\n",
    "                    continue\n",
    "\n",
    "                for table in tables:\n",
    "                    table_id = table.table_id\n",
    "                    data_scan_name = f\"profile-{bigquery_project_id}-{dataset.dataset_id}-{table_id}\".replace(\"_\", \"-\")\n",
    "\n",
    "                    # Create profiling job\n",
    "                    create_profiling_job(dplx_parent, data_scan_name, bigquery_project_id, dataset_id, table_id)\n",
    "\n",
    "                    # Run Profiling Job\n",
    "                    run_dplex_job(\"profiling\", data_scan_name)\n",
    "\n",
    "                    # Update publishing to BQ\n",
    "                    apply_profiling_export_to_bq(data_scan_name, f\"{bigquery_project_id}.{dataset_id}.{table_id}\")\n",
    "\n",
    "                    ## Create Quality  Job\n",
    "\n",
    "                    ## Update publishing DQ to BQ\n",
    "                    # apply_quality_export_to_bq()\n",
    "\n",
    "                    ## Run Quality  Job\n",
    "                    #run_dplex_job(\"quality\", data_q_scan_name)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing project '{bigquery_project_id}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bcc090-9bcd-45f2-a827-15f7e4900876",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.1.2 Run data scan job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5144b47b-4b04-4cf7-9f23-604dce056c82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_dplex_job(job_type, data_scan_name):\n",
    "    try:\n",
    "        run_response = dataplex_client.run_data_scan(\n",
    "            name = f\"projects/{GOOGLE_CLOUD_PROJECT_DATAPLEX}/locations/{DATAPLEX_LOCATION}/dataScans/{data_scan_name}\"\n",
    "        )\n",
    "        print(f\"    - Succesfully started {job_type} job\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"    - Failed to run {job_type} job. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c92736-711d-4dd7-8d62-f6574f657dd6",
   "metadata": {},
   "source": [
    "#### 3.1.3 Create profiling job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bd5fe5a5-d5da-42fc-a0b0-ff4518bf62e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_profiling_job(dplx_parent, data_scan_name, bigquery_project_id, dataset_id, table_id):\n",
    "    bq_uri = f\"//bigquery.googleapis.com/projects/{bigquery_project_id}/datasets/{dataset_id}/tables/{table_id}\"\n",
    "    \n",
    "    # Define the Data Profile Scan configuration\n",
    "    data_scan = dataplex_v1.DataScan(\n",
    "        display_name = data_scan_name,\n",
    "        data={\n",
    "            \"resource\": bq_uri\n",
    "        },\n",
    "        data_profile_spec={\n",
    "            \"sampling_percent\": sampling_percent,\n",
    "            \"post_scan_actions\": {\n",
    "                \"bigquery_export\": {\n",
    "                    \"results_table\": BQ_DPLEX_EXPORT_TABLE\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        execution_spec={\n",
    "            \"trigger\": {\"on_demand\": {}}, \n",
    "        },\n",
    "        labels = {\n",
    "            \"source_project\": bigquery_project_id,\n",
    "            \"source_dataset\": dataset_id,\n",
    "            \"created_by\": \"dplex_at_scale\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Create Profiling Job\n",
    "    try:\n",
    "        request = dataplex_client.create_data_scan(\n",
    "            parent=dplx_parent,\n",
    "            data_scan_id=data_scan_name,\n",
    "            data_scan=data_scan,\n",
    "        )\n",
    "        request.result()\n",
    "        \n",
    "        print(f\"  ** Created Dataplex profiling job for table: {table_id}\")\n",
    "    except google.api_core.exceptions.AlreadyExists:\n",
    "        print(f\"    - Job '{data_scan_name}' already exists. Skipping creation.\")\n",
    "    except Exception as e:\n",
    "        print(f\"    - Failed to create job for table {table_id}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a22b957-b0c2-430e-80bf-571c7be97282",
   "metadata": {},
   "source": [
    "#### 3.1.4 Apply profiling export to job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc31a4cc-0120-4144-9962-a70274826b3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_profiling_export_to_bq(SCAN_ID, table_id):\n",
    "    try:\n",
    "        labels_to_set = {\n",
    "            \"dataplex-dp-published-scan\": SCAN_ID,\n",
    "            \"dataplex-data-documentation-published-location\": DATAPLEX_LOCATION,\n",
    "            \"dataplex-dp-published-project\": GOOGLE_CLOUD_PROJECT_DATAPLEX.lower(),\n",
    "            \"dataplex-dp-published-location\": DATAPLEX_LOCATION,\n",
    "            \"dataplex-data-documentation-published-project\": GOOGLE_CLOUD_PROJECT_DATAPLEX.lower(),\n",
    "            \"dataplex-data-documentation-published-scan\": SCAN_ID.lower(),\n",
    "        }\n",
    "        table = bq_client.get_table(table_id)\n",
    "        if table.labels:\n",
    "            updated_labels = table.labels.copy()\n",
    "            updated_labels.update(labels_to_set)\n",
    "        else:\n",
    "            updated_labels = labels_to_set\n",
    "\n",
    "        table.labels = updated_labels\n",
    "        bq_client.update_table(table, [\"labels\"])\n",
    "        print(f\"    - Successfully updated profile export config for {table_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    - Failed to update publishing job for table {table_id}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b8fc94-56b1-4cb8-8e53-22e6ddc60710",
   "metadata": {},
   "source": [
    "#### 3.1.5 Create quality job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535cff3d-ce8a-4c05-adb3-e02f45a94d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59238c47-27c1-4329-b70e-a310d5d8c836",
   "metadata": {},
   "source": [
    "#### 3.1.6 Apply quality export to job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c022945-cdab-4c5c-89b8-d5d94a986c1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_quality_export_to_bq(SCAN_ID, table_id):\n",
    "    try:\n",
    "        labels_to_set = {\n",
    "            \"dataplex-dq-published-scan\": SCAN_ID.lower(),\n",
    "            \"dataplex-dq-published-project\": GOOGLE_CLOUD_PROJECT_DATAPLEX.lower(),\n",
    "            \"dataplex-dq-published-location\": DATAPLEX_LOCATION\n",
    "        }\n",
    "        table = bq_client.get_table(table_id)\n",
    "        if table.labels:\n",
    "            updated_labels = table.labels.copy()\n",
    "            updated_labels.update(labels_to_set)\n",
    "        else:\n",
    "            updated_labels = labels_to_set\n",
    "\n",
    "        table.labels = updated_labels\n",
    "        bq_client.update_table(table, [\"labels\"])\n",
    "\n",
    "        print(f\"    - Successfully updated quality export config for {table_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    - Failed to update publishing job for table {table_id}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcba326b-e9a3-4c47-a361-f5463b501b41",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3.1.7 Schedule or disable schedule for all both jobs: quality and profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b5567c-dc83-4efa-bfc7-7eb24339d1f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_schedule_table_profiling_jobs( schedule ) :\n",
    "    \"\"\"\n",
    "    Creates Dataplex table profiling jobs for all datasets within a list of BigQuery projects' datasets.\n",
    "\n",
    "    Args:\n",
    "        bigquery_project_ids: A list of BigQuery project IDs to iterate through.\n",
    "        dataplex_project_id: The ID of the Dataplex project.\n",
    "        dataplex_location: The Google Cloud region for the Dataplex jobs (e.g., 'us-central1').\n",
    "    \"\"\"\n",
    "    for bigquery_project_id in GOOGLE_CLOUD_PROJECT_ID_BQ_SOURCE:\n",
    "        print(f\"\\nProcessing BigQuery project: {bigquery_project_id}\")\n",
    "        \n",
    "        dplx_parent = dataplex_client.common_location_path(\n",
    "            GOOGLE_CLOUD_PROJECT_DATAPLEX, DATAPLEX_LOCATION\n",
    "        )\n",
    "        \n",
    "        if schedule:\n",
    "            trigger = { \n",
    "                \"schedule\": { \n",
    "                    \"cron\": DPLEX_CRON_SCHEDULE\n",
    "                }\n",
    "            }\n",
    "            trigger_description = f\"scheduled at {DPLEX_CRON_SCHEDULE}\"\n",
    "        else:\n",
    "            trigger = {\"on_demand\": {}}\n",
    "            trigger_description = f\"disabled scheduling\"\n",
    "            \n",
    "\n",
    "        try:\n",
    "            # Initialize BigQuery client for the current project\n",
    "            bigquery_client = bigquery.Client(project=bigquery_project_id)\n",
    "            datasets = list(bigquery_client.list_datasets())\n",
    "\n",
    "            if not datasets:\n",
    "                print(f\"No datasets found in the BigQuery project '{bigquery_project_id}'.\")\n",
    "                continue\n",
    "\n",
    "            for dataset in datasets:\n",
    "                dataset_id = dataset.dataset_id\n",
    "                time.sleep(0.5)\n",
    "\n",
    "                if len(GOOGLE_CLOUD_DATASET_TEST) > 0:\n",
    "                    if dataset_id in GOOGLE_CLOUD_DATASET_TEST:\n",
    "                        pass\n",
    "                    else:\n",
    "                        print(f\"\\Skipping dataset: {dataset_id}\")\n",
    "                        continue\n",
    "                    \n",
    "                print(f\"\\nProcessing dataset: {dataset_id}\")\n",
    "                tables = list(bigquery_client.list_tables(dataset))\n",
    "\n",
    "                if not tables:\n",
    "                    print(f\"No tables found in dataset: {dataset.dataset_id}\")\n",
    "                    continue\n",
    "\n",
    "                for table in tables:\n",
    "                    table_id = table.table_id\n",
    "                    data_scan_name = f\"{bigquery_project_id}-{dataset.dataset_id}-{table_id}\".replace(\"_\", \"-\")\n",
    "                    scan_name = f\"projects/{GOOGLE_CLOUD_PROJECT_DATAPLEX}/locations/{DATAPLEX_LOCATION}/dataScans\"\n",
    "\n",
    "                    # Update profiling job\n",
    "                    dataplex_client.update_data_scan(\n",
    "                        data_scan = dataplex_v1.DataScan(\n",
    "                            name=f\"{scan_name}/profile-{data_scan_name}\",\n",
    "                            execution_spec={\n",
    "                                \"trigger\": trigger\n",
    "                            },\n",
    "                            data_profile_spec = {}\n",
    "                        ),\n",
    "                        update_mask = field_mask_pb2.FieldMask(\n",
    "                            paths=[\"execution_spec.trigger\"]\n",
    "                        )\n",
    "                    )\n",
    "                    print(f\"Update schedule of profiling job {scan_name}. {trigger_description}\")\n",
    "\n",
    "                    \"\"\"\n",
    "                    # Update quality job\n",
    "                    dataplex_client.update_data_scan(\n",
    "                        data_scan = dataplex_v1.DataScan(\n",
    "                            display_name = f\"quality-{data_scan_name}\",\n",
    "                            execution_spec={\n",
    "                                \"trigger\": dplex_trigger \n",
    "                            },\n",
    "                        ),\n",
    "                        update_mask = [\"execution_spec.trigger\"]\n",
    "                    )\n",
    "                    print(\"Update schedule of quality job {data_scan_name}\")\n",
    "                    \"\"\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing project '{bigquery_project_id}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b61e901-de66-4818-9c56-4756ac4337cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.2 Run the job creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ea24af2e-c567-4973-b65d-cc9a5e676fe7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing BigQuery project: bq-sample-project\n",
      "\n",
      "Processing dataset: bank\n",
      "  ** Created Dataplex profiling job for table: accounts\n",
      "    - Succesfully started profiling job\n",
      "    - Successfully updated profile export config for bq-sample-project.bank.accounts\n",
      "  ** Created Dataplex profiling job for table: customers\n",
      "    - Succesfully started profiling job\n",
      "    - Successfully updated profile export config for bq-sample-project.bank.customers\n",
      "\n",
      "Processing dataset: retail\n",
      "  ** Created Dataplex profiling job for table: products\n",
      "    - Succesfully started profiling job\n",
      "    - Successfully updated profile export config for bq-sample-project.retail.products\n",
      "  ** Created Dataplex profiling job for table: sales\n",
      "    - Succesfully started profiling job\n",
      "    - Successfully updated profile export config for bq-sample-project.retail.sales\n"
     ]
    }
   ],
   "source": [
    "create_dataplex_table_profiling_jobs( )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a9e765-e118-4af9-8c00-f9d0dbe9ae58",
   "metadata": {},
   "source": [
    "## 4. Schedule\n",
    "\n",
    "Wait for job run completion before changing schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "13198834-238b-4a67-840b-a3f6528e8057",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Change input variable schedule as needed. False = ondemand\n",
    "schedule = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c29ce858-7a4a-4242-b065-2c96df53bd51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing BigQuery project: bq-sample-project\n",
      "\n",
      "Processing dataset: bank\n",
      "Update schedule of profiling job projects/bq-dataplex_project/locations/us-central1/dataScans. disabled scheduling\n",
      "Update schedule of profiling job projects/bq-dataplex_project/locations/us-central1/dataScans. disabled scheduling\n",
      "\n",
      "Processing dataset: retail\n",
      "Update schedule of profiling job projects/bq-dataplex_project/locations/us-central1/dataScans. disabled scheduling\n",
      "Update schedule of profiling job projects/bq-dataplex_project/locations/us-central1/dataScans. disabled scheduling\n"
     ]
    }
   ],
   "source": [
    "update_schedule_table_profiling_jobs(schedule = schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acf1e11-f95d-4bed-948a-1f8d27d913cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
